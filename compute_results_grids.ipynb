{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from utils import plot_pct_change\n",
    "from niagara.probabilistic_modeling.optimize_cascade import make_full_data, score_cascade, \\\n",
    "    compute_results_grids, get_expected_uncumulated_costs, smooth_outliers\n",
    "from early_abs_setup import setup_data\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "CASCADES = [\n",
    "    (\"llama_chain\", [0,1]),\n",
    "    (\"llama_chain\", [0,2]),\n",
    "    (\"llama_chain\", [0,3]),\n",
    "    (\"llama_chain\", [0,4]),\n",
    "    (\"llama_chain\", [1,2]),\n",
    "    (\"llama_chain\", [1,3]),\n",
    "    (\"llama_chain\", [1,4]),\n",
    "    (\"llama_chain\", [2,3]),\n",
    "    (\"llama_chain\", [2,4]),\n",
    "    (\"llama_chain\", [3,4]),\n",
    "    (\"qwen_oai_chain\", [0,1]),\n",
    "    (\"qwen_oai_chain\", [0,2]),\n",
    "    (\"qwen_oai_chain\", [0,3]),\n",
    "    (\"qwen_oai_chain\", [1,2]),\n",
    "    (\"qwen_oai_chain\", [1,3]),\n",
    "    (\"qwen_oai_chain\", [2,3])\n",
    "]\n",
    "\n",
    "outlier_stats = []\n",
    "outlier_stats_no = []\n",
    "\n",
    "benchmark_names = [\"mmlu\", \"medmcqa\", \"triviaqa\", \"xsum\", \"gsm8k\", \"truthfulqa\"]\n",
    "\n",
    "metric = 'error'\n",
    "average_train_results = { }\n",
    "average_test_results = { }\n",
    "\n",
    "for cascade in tqdm(CASCADES):\n",
    "    chain_name, model_indices = cascade\n",
    "    model_idx_str = \"\".join([ str(x) for x in model_indices ])\n",
    "\n",
    "    average_train_results[chain_name + model_idx_str] = { name: {} for name in benchmark_names }\n",
    "    average_test_results[chain_name + model_idx_str] = { name: {} for name in benchmark_names }\n",
    "\n",
    "    for benchmark_name in benchmark_names:\n",
    "        ### ### ### ### ### ### ### ###\n",
    "        PROB_MODEL_FILENAME = f\"./optimal_thresholds_data/{benchmark_name}/prob_model_results_{chain_name}.pkl\"\n",
    "        OPT_THOLDS_FILENAME = f\"./optimal_thresholds_data/{benchmark_name}/optimal_tholds_{chain_name}_{model_idx_str}.pkl\"\n",
    "\n",
    "        with open(PROB_MODEL_FILENAME, \"rb\") as file:\n",
    "            prob_model = pickle.load(file)\n",
    "            \n",
    "        with open(OPT_THOLDS_FILENAME, \"rb\") as file:\n",
    "            optimal_thresholds = pickle.load(file)\n",
    "\n",
    "        ###### Evaluate The Cascade ######\n",
    "\n",
    "        lambda_abs_grid = optimal_thresholds[\"lambda_abs_grid\"]\n",
    "        lambda_cost_grid = optimal_thresholds[\"lambda_cost_grid\"]\n",
    "        error_type = optimal_thresholds[\"error_type\"]\n",
    "        T_2d, S_2d = [ optimal_thresholds[\"early_abs\"][x] for x in [\"T\",\"S\"] ]\n",
    "        T_2d_no, S_2d_no = [ optimal_thresholds[\"final_model_abs\"][x] for x in [\"T\",\"S\"] ]\n",
    "\n",
    "        ### Get The Train and Test Data\n",
    "\n",
    "        all_setup_data = setup_data(NAME=benchmark_name, CHAIN_NAME=chain_name)\n",
    "\n",
    "        raw_model_costs = { \n",
    "            model_name: all_setup_data['chain'].models[i].cpm_tokens \n",
    "                for i, model_name in enumerate(all_setup_data['chain'].model_names) \n",
    "        }\n",
    "        results_train = all_setup_data['raw_results']['train']\n",
    "        results_test = all_setup_data['raw_results']['test']\n",
    "        expected_uncumulated_costs_train = get_expected_uncumulated_costs(raw_model_costs, results_train)\n",
    "        expected_uncumulated_costs_test = get_expected_uncumulated_costs(raw_model_costs, results_test)\n",
    "\n",
    "        calibrated_conf_train = all_setup_data['calibrated_conf']['train']\n",
    "        calibrated_conf_test = all_setup_data['calibrated_conf']['test']\n",
    "        corr_train = all_setup_data['corr']['train']\n",
    "        corr_test = all_setup_data['corr']['test']\n",
    "        fit_stats = all_setup_data['logreg_fit_stats']\n",
    "\n",
    "        test_data = {\n",
    "            'calib_conf': make_full_data(calibrated_conf_test), \n",
    "            'corr': make_full_data(corr_test)\n",
    "        }\n",
    "\n",
    "        train_data = {\n",
    "            'calib_conf': make_full_data(calibrated_conf_train), \n",
    "            'corr': make_full_data(corr_train)\n",
    "        }\n",
    "\n",
    "        ### Smooth the optimal thresholds\n",
    "\n",
    "        outlier_threshold = 10\n",
    "\n",
    "        if outlier_threshold is not None:\n",
    "            T_2d, outliers_T_2d = smooth_outliers(T_2d, r=outlier_threshold)\n",
    "            S_2d, outliers_S_2d = smooth_outliers(S_2d, r=outlier_threshold)\n",
    "            T_2d_no, outliers_T_2d_no = smooth_outliers(T_2d_no, r=outlier_threshold)\n",
    "            S_2d_no, outliers_S_2d_no = smooth_outliers(S_2d_no, r=outlier_threshold)\n",
    "\n",
    "            print(f\"{np.mean(outliers_T_2d)} outliers smoothed! (early T)\")\n",
    "            print(f\"{np.mean(outliers_S_2d)} outliers smoothed! (early S)\")\n",
    "            print(f\"{np.mean(outliers_T_2d_no)} outliers smoothed! (final T)\")\n",
    "            print(f\"{np.mean(outliers_S_2d_no)} outliers smoothed! (final S)\")\n",
    "\n",
    "            outlier_stats += [ \n",
    "                np.mean(outliers_T_2d),\n",
    "                np.mean(outliers_S_2d),\n",
    "            ]\n",
    "\n",
    "            outlier_stats_no += [\n",
    "                np.mean(outliers_T_2d_no),\n",
    "                np.mean(outliers_S_2d_no)\n",
    "            ]\n",
    "\n",
    "        ### Compute Results Grids\n",
    "\n",
    "        results_grids_train_early_abs = compute_results_grids(\n",
    "            model_indices=model_indices,\n",
    "            T_2d=T_2d,\n",
    "            S_2d=S_2d,\n",
    "            lambda_cost_grid=lambda_cost_grid,\n",
    "            lambda_abs_grid=lambda_abs_grid,\n",
    "            data=train_data,\n",
    "            expected_uncumulated_costs=expected_uncumulated_costs_train,\n",
    "            error_type=error_type\n",
    "        )\n",
    "\n",
    "        results_grids_train_final_model_abs = compute_results_grids(\n",
    "            model_indices=model_indices,\n",
    "            T_2d=T_2d_no,\n",
    "            S_2d=S_2d_no,\n",
    "            lambda_cost_grid=lambda_cost_grid,\n",
    "            lambda_abs_grid=lambda_abs_grid,\n",
    "            data=train_data,\n",
    "            expected_uncumulated_costs=expected_uncumulated_costs_train,\n",
    "            error_type=error_type\n",
    "        )\n",
    "\n",
    "        results_grids_test_early_abs = compute_results_grids(\n",
    "            model_indices=model_indices,\n",
    "            T_2d=T_2d,\n",
    "            S_2d=S_2d,\n",
    "            lambda_cost_grid=lambda_cost_grid,\n",
    "            lambda_abs_grid=lambda_abs_grid,\n",
    "            data=test_data,\n",
    "            expected_uncumulated_costs=expected_uncumulated_costs_test,\n",
    "            error_type=error_type\n",
    "        )\n",
    "\n",
    "        results_grids_test_final_model_abs = compute_results_grids(\n",
    "            model_indices=model_indices,\n",
    "            T_2d=T_2d_no,\n",
    "            S_2d=S_2d_no,\n",
    "            lambda_cost_grid=lambda_cost_grid,\n",
    "            lambda_abs_grid=lambda_abs_grid,\n",
    "            data=test_data,\n",
    "            expected_uncumulated_costs=expected_uncumulated_costs_test,\n",
    "            error_type=error_type\n",
    "        )\n",
    "\n",
    "        plot_pct_change(\n",
    "            results_grids_test_early_abs, \n",
    "            results_grids_test_final_model_abs, \n",
    "            lambda_cost_grid,\n",
    "            lambda_abs_grid,\n",
    "            filename=f\"pct_chg_heatmap_{benchmark_name}_{chain_name}_{model_idx_str}_{metric}_outlier={outlier_threshold}.pdf\",\n",
    "            save_fig=True\n",
    "        )\n",
    "\n",
    "        early_abs_val_train = np.mean(results_grids_train_early_abs[metric])\n",
    "        final_model_abs_val_train = np.mean(results_grids_train_final_model_abs[metric])\n",
    "\n",
    "        early_abs_val_test = np.mean(results_grids_test_early_abs[metric])\n",
    "        final_model_abs_val_test = np.mean(results_grids_test_final_model_abs[metric])\n",
    "\n",
    "        average_train_results[chain_name + model_idx_str][benchmark_name]['early_abs'] = early_abs_val_train\n",
    "        average_train_results[chain_name + model_idx_str][benchmark_name]['final_model_abs'] = final_model_abs_val_train\n",
    "        average_test_results[chain_name + model_idx_str][benchmark_name]['early_abs'] = early_abs_val_test\n",
    "        average_test_results[chain_name + model_idx_str][benchmark_name]['final_model_abs'] = final_model_abs_val_test\n",
    "\n",
    "### Save to file\n",
    "with open(f\"./performance_data/overall_results_{metric}_outlier={outlier_threshold}.pkl\", \"wb\") as file:\n",
    "    pickle.dump({\n",
    "        \"train\": average_train_results,\n",
    "        \"test\": average_test_results\n",
    "    }, file)\n",
    "\n",
    "with open(f\"./performance_data/outlier_stats_outlier={outlier_threshold}.pkl\", \"wb\") as file:\n",
    "    pickle.dump({\n",
    "        \"early_abs\": outlier_stats,\n",
    "        \"final_model_abs\": outlier_stats_no\n",
    "    }, file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
